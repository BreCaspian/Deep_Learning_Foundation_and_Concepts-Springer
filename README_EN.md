<h1 style="text-align: center;">Deep Learning: Foundations and Concepts</h1>

<p align="center">
  <b>Language | </b>
  <b>English</b> |
  <a href="README.md">‰∏≠Êñá</a>
</p>

This repository contains supplementary resources, exercise materials, and solutions for the book *Deep Learning: Foundations and Concepts*, written by Christopher M. Bishop and Hugh Bishop, published by Springer in 2023.

## üóÇÔ∏è Repository Structure

The repository is organized as follows:

- **Book/** - Materials related to the book
  - **Book_PDF/** - PDF version of the book
  - **Book_PNG/** - PNG version of the book

- **Code/** - Code examples and exercise notebooks
  - Includes Jupyter notebooks covering topics such as handwritten digit recognition, PyTorch basics, neural network implementations (including UNet architecture), etc.

- **Data/** - Datasets for exercises
  - **MNIST/** - MNIST dataset for handwritten digit recognition

- **Figure/** - All figures and illustrations from the book

- **Solutions/** - Solutions to the book‚Äôs exercises with mathematical derivations
  - Includes PDF files on topics such as probability theory, standard distributions, single-layer networks, Transformers, Variational Autoencoders, etc.

## üìö About the Book

*Deep Learning: Foundations and Concepts* is a comprehensive textbook that covers both the theoretical foundations and practical applications of deep learning. It is authored by Christopher M. Bishop (Microsoft Technical Fellow, Director of Microsoft Research AI4Science, Fellow of Darwin College Cambridge, Fellow of the Royal Academy of Engineering, Fellow of the Royal Society) and Hugh Bishop (Applied Scientist at Wayve, London, focusing on end-to-end deep learning for autonomous driving).

### üåü Key Features

- Comprehensive coverage from basic concepts to advanced architectures
- Clear explanations through text, diagrams, mathematical formulas, and pseudocode
- Self-contained introduction to probability theory
- In-depth exploration of modern architectures (MLP, CNN, RNN, Transformer, GNN)
- Coverage of attention mechanisms, GANs, VAEs, transfer learning, and contrastive learning
- Applications in computer vision, natural language processing, speech recognition, protein structure prediction, and medical diagnostics
- Especially praised for its clear explanation of Transformers and large language models

### üéØ Target Audience

- Beginners and experienced practitioners in machine learning
- Academic researchers and students (undergraduate or graduate level)
- Self-learners interested in deep learning theory
- Professionals seeking a solid foundation for future research or specialization

## üìñ Recommended Prerequisite Knowledge

- Basics of linear algebra (matrix operations, eigenvalues)
- Basics of probability theory (Bayes' theorem, conditional probability)
- Basic calculus (gradients, partial derivatives)

## üõ†Ô∏è How to Use This Repository

1. Start with the exercise notebooks in the `Code/` directory to gain hands-on experience
2. After attempting the exercises on your own, refer to the mathematical derivations in the `Solutions/` directory
3. Implement practical solutions using the datasets in the `Data/` directory
4. Consult the charts in the `Figure/` directory for intuitive explanations

## üìë Citation

If you use these materials in your research or projects, please cite the original book:

```
Bishop, C. M., & Bishop, H. (2023). Deep Learning: Foundations and Concepts. Springer.
```

## üîó Additional Resources

- [Official Book Website](https://www.bishopbook.com/)
- [Springer Link](https://link.springer.com/book/10.1007/978-3-031-45468-4)
- [Microsoft Research Publications](https://www.microsoft.com/en-us/research/publication/deep-learning-foundations-and-concepts/)

## üìú License Information

This repository is for educational purposes only. Please respect the copyright of the original materials. All content is shared under fair use principles for academic and research purposes.
